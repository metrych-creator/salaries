{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b496f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pip install openpyxl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "797e9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf998b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Train_rev1.csv', index_col='Id')\n",
    "df_test = pd.read_csv('data/Test_rev1.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065321d-3e4c-4859-bcea-c506d0264b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape: \", df.shape)\n",
    "print(\"Test data shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc726b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ef045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7383c5",
   "metadata": {},
   "source": [
    "# 1. Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['SalaryRaw', 'LocationRaw'], inplace=True)\n",
    "df_test.drop(columns=['LocationRaw'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape: \", df.shape)\n",
    "print(\"Test data shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bae63",
   "metadata": {},
   "source": [
    "# 2. Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4036bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values:')\n",
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc875107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_train_test(train_df, test_df):\n",
    "    # Store the fill values from train\n",
    "    fill_values = {}\n",
    "    \n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].dtype == 'O':  # object/string\n",
    "            fill_values[col] = train_df[col].mode()[0]\n",
    "        else:  # numbers\n",
    "            fill_values[col] = train_df[col].mean()\n",
    "    \n",
    "    # Fill train and test with the same values\n",
    "    train_filled = train_df.fillna(fill_values)\n",
    "    test_filled = test_df.fillna(fill_values)\n",
    "    \n",
    "    return train_filled, test_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bcae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test = fill_missing_train_test(df, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321317d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f69de",
   "metadata": {},
   "source": [
    "# 3. Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa60958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset duplicates BEFORE removing: \", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Train dataset duplicates AFTER removing: \", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa8a2e-6560-4208-8a3f-ef86300dc686",
   "metadata": {},
   "source": [
    "# 4a. Geostandarization - web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ea51b-cea0-410a-9148-1b1c0881d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests -q\n",
    "%pip install beautifulsoup4 -q\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bd553-ea3f-4c81-b893-63ac19a11731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_text(text):\n",
    "    match = re.search(r'(\\d{1,3}(?:,\\d{3})*)', text)\n",
    "    return int(match.group(1).replace(',', '')) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e698b-3d29-4a91-aabc-ac897c3efa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_population_from_table(table):\n",
    "    for header_row in table.select('tr:has(th)'):\n",
    "        th = header_row.select_one('th')\n",
    "        if 'Population' not in th.get_text():\n",
    "            continue\n",
    "\n",
    "        # next <tr> sibling (population data may be here)\n",
    "        next_row = header_row.find_next_sibling('tr')\n",
    "\n",
    "\n",
    "        # 1. population in the same row\n",
    "        td = header_row.select_one('td')\n",
    "        if td:\n",
    "            val = extract_number_from_text(td.get_text())\n",
    "            if val:\n",
    "                return val\n",
    "            \n",
    "        # 2. population in the next row <td>\n",
    "        if next_row and next_row.select_one('td'):\n",
    "            val = extract_number_from_text(next_row.select_one('td').get_text())\n",
    "            if val:\n",
    "                return val\n",
    "\n",
    "        # 3. multiple population years (bulleted list)\n",
    "        if next_row and re.match(r'\\s*•\\s*\\d{4}', next_row.get_text()):\n",
    "            # select all following <tr> until a break\n",
    "            for tr in header_row.find_all_next('tr'):\n",
    "                val = extract_number_from_text(tr.get_text())\n",
    "                if val:\n",
    "                    last_val = val\n",
    "            return last_val\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280282b-f985-47a2-babb-3189de1c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, headers, retries=3, delay_range=(1, 3)):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            if response.status_code == 404:\n",
    "                return None\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        time.sleep(random.uniform(*delay_range))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83f26a-9798-477d-8591-ce2ad615a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_infobox_table(url, headers, class_name='infobox'):\n",
    "    page = get_page(url, headers)\n",
    "    if not page:\n",
    "        return None\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    return soup.find('table', class_=class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf870b6-773b-4511-a1dc-b902612dcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population_for_city(city, headers):\n",
    "    urls = [\n",
    "        f'https://en.wikipedia.org/wiki/{city}',\n",
    "        f'https://en.wikipedia.org/wiki/{city}_(county)'\n",
    "    ]\n",
    "    for url in urls:\n",
    "        table = fetch_infobox_table(url, headers)\n",
    "        if table:\n",
    "            pop = select_population_from_table(table)\n",
    "            if pop:\n",
    "                return pop\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4330aa-700e-4a08-9e81-37abd7bef37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population_for_location():\n",
    "    headers = {\"User-Agent\": \"LocationWebScrapper\"}\n",
    "\n",
    "    for city in cities:\n",
    "        population = population_cache.get(city)\n",
    "        if population is None:\n",
    "            population = get_population_for_city(city, headers)\n",
    "            if population:\n",
    "                population_cache[city] = population\n",
    "            else:\n",
    "                not_working.add(city)\n",
    "        print(f\"{city}: {population}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793bb9e-c8ec-4cee-8e7d-a4505a68ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_info(df):\n",
    "    print(f\"Missing data in population of location: {round(df[df['LocationPopulation'].isna()]['LocationNormalized'].count() / len(df) * 100, 2)}%, {df[df['LocationPopulation'].isna()]['LocationNormalized'].count()} cases\")\n",
    "    print()\n",
    "    print(df[df['LocationPopulation'].isna()]['LocationNormalized'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e368a-fb94-4c9f-b8b2-eac4846b705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_working = set()\n",
    "# population_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7ba15-24a8-4067-bcfb-9abb92bd3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/population_cache.json', 'r', encoding='utf-8') as f:\n",
    "    population_cache = json.load(f)\n",
    "\n",
    "cities = df['LocationNormalized'].unique().tolist()\n",
    "not_working = [city for city in cities if city not in population_cache]\n",
    "\n",
    "df['LocationPopulation'] = df['LocationNormalized'].str.strip().map(lambda x: population_cache.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206200e-2def-4ab3-95b5-6fd4eb51bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df)\n",
    "print()\n",
    "print('not working cities: ', len(not_working))\n",
    "print('cities in cache: ', len(population_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8a183-7a1c-48d5-830e-ea8d02b62b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_population_for_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68169def-fcd6-40d9-84e5-531027d89376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/population_cache.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(population_cache, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c9256",
   "metadata": {},
   "source": [
    "# 4b. Geostandarization - using common datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b28cea",
   "metadata": {},
   "source": [
    "## 4.1. Get population data from geonames dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f131deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting data only for GB - turn on once (long)\n",
    "# cols = [\n",
    "#     'geonameid','name','asciiname','alternatenames','lat','lon',\n",
    "#     'feature_class','feature_code','country_code','cc2','admin1',\n",
    "#     'admin2','admin3','admin4','population','elevation','dem','tz','moddate'\n",
    "# ]\n",
    "\n",
    "# geonames = pd.read_csv(\n",
    "#     \"allCountries.txt\",\n",
    "#     sep=\"\\t\",\n",
    "#     names=cols,\n",
    "#     usecols=['asciiname', 'alternatenames', 'country_code', 'feature_code', 'feature_class', 'admin1', 'admin2', 'admin3', 'lon', 'lat', 'population'],\n",
    "#     dtype=str,\n",
    "#     header=None\n",
    "# )\n",
    "\n",
    "# geonames_gb = geonames[geonames['country_code'] == 'GB'].copy().reset_index(drop=True)\n",
    "# geonames_gb = geonames_gb[geonames_gb['feature_class'].isin(['P', 'A'])].reset_index()\n",
    "# geonames_gb.loc[geonames_gb['feature_code'] == 'PCLI', 'asciiname'] = 'UK'\n",
    "# geonames_gb.to_csv('geonames_gb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames_gb = pd.read_csv('geo_datasets/geonames_gb.csv')\n",
    "geonames_gb.rename(columns={'asciiname': 'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6be49f",
   "metadata": {},
   "source": [
    "## 4.2. Get population for all locations where it is directly possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get population for locations\n",
    "pop_dict = geonames_gb['population'].copy()\n",
    "pop_dict = geonames_gb.set_index(geonames_gb['name'].str.lower().str.strip())['population'].to_dict()\n",
    "\n",
    "pop_dict_test = geonames_gb['population'].copy()\n",
    "pop_dict_test = geonames_gb.set_index(geonames_gb['name'].str.lower().str.strip())['population'].to_dict()\n",
    "\n",
    "df['LocationPopulation'] = df['LocationNormalized'].str.lower().str.strip().map(lambda x: pop_dict.get(x))\n",
    "df_test['LocationPopulation'] = df_test['LocationNormalized'].str.lower().str.strip().map(lambda x: pop_dict_test.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missing_info(df):\n",
    "    print(f\"Missing data in population of location: {round(df[df['LocationPopulation'].isna()]['LocationNormalized'].count() / len(df) * 100, 2)}%, {df[df['LocationPopulation'].isna()]['LocationNormalized'].count()} cases\")\n",
    "    print()\n",
    "    print(df[df['LocationPopulation'].isna()]['LocationNormalized'].value_counts()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22df645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83a5a1",
   "metadata": {},
   "source": [
    "## 4.3. Remove directions and assign population to other fitting names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_population(df, location_col='LocationNormalized', pop_col='LocationPopulation', pop_dict=None):\n",
    "    directions = ['North', 'South', 'East', 'West', 'Central']\n",
    "\n",
    "    df[location_col] = df[location_col].replace(directions, '', regex=True).str.strip()\n",
    "\n",
    "    missing_mask = df[pop_col].isna()\n",
    "    missing_locations = df.loc[missing_mask, location_col].str.lower().str.strip()\n",
    "\n",
    "    pop_dict_missing = {loc: pop_dict.get(loc, np.nan) for loc in missing_locations}\n",
    "    df.loc[missing_mask, pop_col] = missing_locations.map(pop_dict_missing)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77824ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_missing_population(df, pop_dict=pop_dict)\n",
    "df_test = fill_missing_population(df_test, pop_dict=pop_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a5c6a",
   "metadata": {},
   "source": [
    "## 4.4. Find population for Midlands in NUT regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove locations out of GB\n",
    "uk_lat_mask = (geonames_gb['lat'] >= 49) & (geonames_gb['lat'] <= 61)\n",
    "uk_lon_mask = (geonames_gb['lon'] >= -10) & (geonames_gb['lon'] <= 2)\n",
    "geonames_gb = geonames_gb[(geonames_gb['country_code'] == 'GB') & (uk_lat_mask) & (uk_lon_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152634f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_excel(\"geo_datasets/NUTS.xlsx\")\n",
    "nuts['NUTS118NM'] = nuts['NUTS118NM'].str.replace('(England)', '', regex=False).str.strip()\n",
    "nuts = nuts.rename(columns={'NUTS118NM': 'name', 'LONG': 'lon', 'LAT': 'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the closest point in geonames in nuts\n",
    "from scipy.spatial import cKDTree\n",
    "tree = cKDTree(geonames_gb[['lat', 'lon']].values)\n",
    "nuts_coords = nuts[['lat', 'lon']].values\n",
    "\n",
    "distances, indices = tree.query(nuts_coords, k=1)  # k=1 -> 1 neighbour\n",
    "\n",
    "nuts['population'] = geonames_gb.iloc[indices]['population'].values\n",
    "nuts_population = dict(zip(nuts['name'], nuts['population']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f04a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Counter\n",
    "\n",
    "# combine West and East Midlands\n",
    "nuts_population = {**{k: v for k, v in nuts_population.items() if 'Midlands' not in k},\n",
    "                 **{'Midlands': sum(v for k, v in nuts_population.items() if 'Midlands' in k)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d190a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute nuts locations\n",
    "def impute_nuts_location(df, nuts_population):\n",
    "    population_from_dict = df['LocationNormalized'].map(nuts_population)\n",
    "    mask = ((df['LocationPopulation'].isnull()) | (df['LocationPopulation'] == 0)) & population_from_dict.notnull()\n",
    "    df.loc[mask, 'LocationPopulation'] = population_from_dict[mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c85f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute_nuts_location(df, nuts_population)\n",
    "df_test = impute_nuts_location(df_test, nuts_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d35f5a",
   "metadata": {},
   "source": [
    "## 4.5. Cast rest of cases as 'UK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_uk_population(df):\n",
    "    mask = df['LocationPopulation'].isna() | (df['LocationPopulation'] == 0)\n",
    "    uk_pop = df.loc[df['LocationNormalized'].str.lower().eq('uk'), 'LocationPopulation'].dropna().iloc[0] if any(df['LocationNormalized'].str.lower().eq('uk')) else np.nan\n",
    "    df.loc[mask, ['LocationNormalized', 'LocationPopulation']] = ['UK', uk_pop]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ea390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute_uk_population(df)\n",
    "df_test = impute_uk_population(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missing_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LocationPopulation'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88225a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['LocationNormalized'], inplace=True)\n",
    "df_test.drop(columns=['LocationNormalized'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10942281-bad0-45d6-99db-4869a76753a5",
   "metadata": {},
   "source": [
    "# Text - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5099241-d35c-4ee9-b8ac-268e24017a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk -q\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "%pip install gensim -q\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb74743-6e27-470e-bd94-5a51d4f6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a7a9d-7292-426c-a405-91291cbc36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average vectors\n",
    "def document_vector(word_list, model, vector_size):\n",
    "    # Initialize a zero vector\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    \n",
    "    # Sum the vectors of all words in the text\n",
    "    for word in word_list:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "            \n",
    "    # Return the average vector\n",
    "    if count != 0:\n",
    "        return vector / count\n",
    "    else:\n",
    "        # Return the zero vector if no words were found in the vocabulary\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ba2c2-0b03-404c-8772-3ee8ad9b5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(df, title_col='Title', desc_col='FullDescription', \n",
    "                   vector_size=50, window=5, min_count=5, sg=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on titles and descriptions from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        title_col (str): Name of the column with titles.\n",
    "        desc_col (str): Name of the column with descriptions.\n",
    "        vector_size (int): Dimensionality of the output vectors.\n",
    "        window (int): Context window size.\n",
    "        min_count (int): Minimum word frequency to consider.\n",
    "        sg (int): 0 = CBOW, 1 = Skip-gram.\n",
    "    \n",
    "    Returns:\n",
    "        gensim.models.Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "\n",
    "    titles = df[title_col].apply(tokenize_text).tolist()\n",
    "    print(f\"Tokenized {len(titles)} titles.\")\n",
    "\n",
    "    descriptions = df[desc_col].apply(tokenize_text).tolist()\n",
    "    print(f\"Tokenized {len(descriptions)} descriptions.\")\n",
    "\n",
    "    all_sentences = titles + descriptions\n",
    "    print(f\"Total sentences for training: {len(all_sentences)}\")\n",
    "    \n",
    "    \n",
    "    workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=all_sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7637cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = train_word2vec(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text_columns(df, w2v_model, vector_size, title_col='Title', desc_col='FullDescription'):\n",
    "    \n",
    "    titles = df[title_col].apply(lambda x: document_vector(tokenize_text(x), w2v_model, vector_size))\n",
    "    descriptions = df[desc_col].apply(lambda x: document_vector(tokenize_text(x), w2v_model, vector_size))\n",
    "    \n",
    "    title_df = pd.DataFrame(titles.tolist(), index=df.index).add_prefix(f'{title_col}_vec_')\n",
    "    desc_df = pd.DataFrame(descriptions.tolist(), index=df.index).add_prefix(f'{desc_col}_vec_')\n",
    "    \n",
    "    return pd.concat([title_df, desc_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_w2v = vectorize_text_columns(df, w2v_model, vector_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edcc7c-9a10-4a5c-a0f7-81b9febfd5ae",
   "metadata": {},
   "source": [
    "# Text - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17bf258-9f33-43e4-8b93-957bf786d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers -q\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a8cea-9529-4015-8fad-8f5ebfcb09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaFeatureDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f6add-4ea1-44af-b854-a70bd723a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cls_vectors(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Converts specified text columns in a DataFrame into Word2Vec vector representations.    \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_cls_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Extraction [CLS] RoBERTa\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            cls_vector = outputs.last_hidden_state[:, 0, :]\n",
    "            \n",
    "            all_cls_vectors.append(cls_vector.cpu().numpy())\n",
    "\n",
    "    final_vector_array = np.concatenate(all_cls_vectors, axis=0)\n",
    "    return final_vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14053e30-5cef-47e2-baf5-e81b4842cdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aaee74a0744242b81dc45880a41b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c8ab149094491b8548f4a4833963be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b61016cdf9497dbeb5ab140d106a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de4f3f941944c768e111d770d6dd596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb69492d00c844fc870dbe92c9a8d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6104341b9e364c2f90b5ae747efbef41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1a4ecf7-4a1b-4443-967a-3e43c47d3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e46244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roberta_features(df, model, device, title_col='Title', desc_col='FullDescription', batch_size=16):\n",
    "    \"\"\"\n",
    "    Extracts CLS token vectors from a RoBERTa model for combined title and description text.\n",
    "    \"\"\"\n",
    "    # Combine title and description with [SEP] token\n",
    "    texts_concat = df[title_col] + ' [SEP] ' + df[desc_col]\n",
    "    texts_list = texts_concat.tolist()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = RobertaFeatureDataset(texts_list)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Extract CLS vectors\n",
    "    final_vector_array = extract_cls_vectors(model, data_loader, device)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_df = pd.DataFrame(final_vector_array, index=df.index)\n",
    "    feature_df.columns = [f'cls_{i}' for i in range(final_vector_array.shape[1])]\n",
    "    \n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e73800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_w2v_roberta = extract_roberta_features(df, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7b531",
   "metadata": {},
   "source": [
    "# 5. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9abe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(df, test_size=0.3, random_state=42)\n",
    "test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b920781-6aee-45e9-b382-948ac10263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "texts_train = texts_w2v.loc[train.index]\n",
    "texts_val = texts_w2v.loc[val.index]\n",
    "texts_train.to_pickle('data/texts_w2v_train.pkl')\n",
    "texts_val.to_pickle('data/texts_w2v_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecc191-682b-4827-890e-c977f3dd56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta\n",
    "texts_roberta_train = texts_w2v_roberta.loc[train.index]\n",
    "texts_roberta_test  = texts_w2v_roberta.loc[val.index]\n",
    "texts_roberta_train.to_parquet('data/texts_roberta_train.parquet', index=True)\n",
    "texts_roberta_test.to_parquet('data/texts_roberta_test.parquet',  index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a04e3",
   "metadata": {},
   "source": [
    "# Text - Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc229695-cee3-4544-ac30-19b9433c8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005a456-808b-4e7d-ba19-25bc397d60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tfidf(df, n_grams):\n",
    "    tfidf_description = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=n_grams)\n",
    "    tfidf_title = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=n_grams)\n",
    "\n",
    "    X_description = tfidf_description.fit_transform(df[\"FullDescription\"])\n",
    "    X_title = tfidf_title.fit_transform(df[\"Title\"])\n",
    "\n",
    "    return hstack([X_description, X_title]), tfidf_description, tfidf_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae80d2c-4e1b-4fcc-8397-346a9fb48821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tfidf(df, tfidf_description, tfidf_title):\n",
    "    X_description = tfidf_description.transform(df[\"FullDescription\"])\n",
    "    X_title = tfidf_title.transform(df[\"Title\"])\n",
    "\n",
    "    return hstack([X_description, X_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a54ca-0d95-4de0-aad7-484791fbb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, tfidf_description, tfidf_title = prepare_tfidf(train, n_grams=(1, 1))\n",
    "print('train completed')\n",
    "X_val_text = transform_tfidf(val, tfidf_description, tfidf_title)\n",
    "print('val completed')\n",
    "X_test_text = transform_tfidf(test, tfidf_description, tfidf_title)\n",
    "print('tdidf test completed')\n",
    "\n",
    "# dimenshion reduction\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "\n",
    "X_train_text = svd.fit_transform(X_train_text)\n",
    "print('svd train completed')\n",
    "X_val_text = svd.transform(X_val_text)\n",
    "print('svd val completed')\n",
    "X_test_text = svd.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb212d-6a36-4799-bcf3-c9c247cb630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "ngram = 'uni'  # 'uni', 'bi', 'tri'...\n",
    "np.save(f\"data/X_train_text_{ngram}.npy\", X_train_text)\n",
    "np.save(f\"data/X_val_text_{ngram}.npy\", X_val_text)\n",
    "np.save(f\"data/X_test_text_{ngram}.npy\", X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61fbe4-af65-4128-b911-e7974d0e7b7b",
   "metadata": {},
   "source": [
    "# Text - Combine Tf-idf & Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41679e",
   "metadata": {},
   "source": [
    "The column 'Title' is going to be transformed by tf-idf (uni-grams) and 'Description' by Roberta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccf316-d99c-4ce9-9e25-17cd19a93b0c",
   "metadata": {},
   "source": [
    "Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13f06a27-ad99-481f-825e-bfb16c6cdded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction [CLS] RoBERTa: 100%|██████████| 10709/10709 [21:40<00:00,  8.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_ds = RobertaFeatureDataset(train['FullDescription'].tolist())\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=False)\n",
    "train_vectors = extract_cls_vectors(model, train_dl, device)\n",
    "desc_roberta_train = pd.DataFrame(train_vectors, index=train.index)\n",
    "desc_roberta_train.columns = [f'cls_{i}' for i in range(train_vectors.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd644fbb-e8e2-4a20-9ed9-21233d8a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_roberta_train.to_parquet(\"data/desc_roberta_train.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bde6299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction [CLS] RoBERTa: 100%|██████████| 4590/4590 [09:14<00:00,  8.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# val\n",
    "val_ds = RobertaFeatureDataset(val['FullDescription'].tolist())\n",
    "val_dl = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "val_vectors = extract_cls_vectors(model, val_dl, device)\n",
    "desc_roberta_val = pd.DataFrame(val_vectors, index=val.index)\n",
    "desc_roberta_val.columns = [f'cls_{i}' for i in range(val_vectors.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93f2cbc6-58b0-4c1d-8884-bfc8901e81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_roberta_val.to_parquet(\"data/desc_roberta_val.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_ds = RobertaFeatureDataset(test['FullDescription'].tolist())\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "test_vectors = extract_cls_vectors(model, test_dl, device)\n",
    "desc_roberta_test = pd.DataFrame(test_vectors, index=test.index)\n",
    "desc_roberta_test.columns = [f'cls_{i}' for i in range(test_vectors.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72f57884-ed2b-4e1e-a17e-f44b92794cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_roberta_test.to_parquet(\"data/desc_roberta_test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3b97c7-80ff-458f-89af-f70202cdf4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read description\n",
    "X_train_desc = pd.read_parquet(\"data/desc_roberta_train.parquet\")\n",
    "X_val_desc = pd.read_parquet(\"data/desc_roberta_val.parquet\")\n",
    "X_test_desc = pd.read_parquet(\"data/desc_roberta_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0b6b0-e679-4b32-93ea-2729641905c1",
   "metadata": {},
   "source": [
    "Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dded364-700f-4da8-ab9c-b56ad350cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_title = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 1))\n",
    "X_train_title = tfidf_title.fit_transform(train[\"Title\"])\n",
    "X_val_title  = tfidf_title.transform(val[\"Title\"])\n",
    "X_test_title  = tfidf_title.transform(test[\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "\n",
    "X_train_title = svd.fit_transform(X_train_title)\n",
    "print('train svd completed')\n",
    "X_val_title = svd.transform(X_val_title)\n",
    "print('val svd completed')\n",
    "X_test_title = svd.transform(X_test_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14471f2e-65a5-4f4c-bea6-06bc06bfd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = np.load(\"data/X_train_title.npy\")\n",
    "X_val_title = np.load(\"data/X_val_title.npy\")\n",
    "X_test_title = np.load(\"data/X_test_title.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69926f82",
   "metadata": {},
   "source": [
    "Combine & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e43cd6-b4da-4a5c-9d34-227bb43cc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_combined = np.concatenate([X_train_desc, X_train_title], axis=1)\n",
    "val_text_combined = np.concatenate([X_val_desc, X_val_title], axis=1)\n",
    "test_text_combined = np.concatenate([X_test_desc, X_test_title], axis=1)\n",
    "\n",
    "train_text_combined_df = pd.DataFrame(train_text_combined)\n",
    "val_text_combined_df = pd.DataFrame(val_text_combined)\n",
    "test_text_combined_df = pd.DataFrame(test_text_combined)\n",
    "\n",
    "train_text_combined_df.to_parquet('data/texts_uni_roberta_train.parquet', index=True)\n",
    "val_text_combined_df.to_parquet('data/texts_uni_roberta_val.parquet',  index=True)\n",
    "test_text_combined_df.to_parquet('data/texts_uni_roberta_test.parquet',  index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80709e3b",
   "metadata": {},
   "source": [
    "# 6. One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most common source in category group\n",
    "category_to_source = train.groupby('Category')['SourceName'].agg(lambda x: x.mode()[0]).to_dict()\n",
    "train['SourceName'] = train['Category'].map(category_to_source)\n",
    "val['SourceName'] = val['Category'].map(category_to_source)\n",
    "df_test['SourceName'] = df_test['Category'].map(category_to_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns = ['ContractType', 'ContractTime', 'Category', 'SourceName'], drop_first=True, dtype=int)\n",
    "val = pd.get_dummies(val, columns = ['ContractType', 'ContractTime', 'Category', 'SourceName'], drop_first=True, dtype=int)\n",
    "test = pd.get_dummies(df_test, columns = ['ContractType', 'ContractTime', 'Category', 'SourceName'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376308cb",
   "metadata": {},
   "source": [
    "# 7. Target Encoding - mean salary of company instead of company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8aed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining companies by two first words\n",
    "train['CompanyPrefix'] = train['Company'].apply(lambda x: ' '.join(str(x).split()[:2]))\n",
    "val['CompanyPrefix'] = val['Company'].apply(lambda x: ' '.join(str(x).split()[:2]))\n",
    "test['CompanyPrefix'] = test['Company'].apply(lambda x: ' '.join(str(x).split()[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean salary by company\n",
    "mean_company = train.groupby('CompanyPrefix')['SalaryNormalized'].mean()\n",
    "train['CompanyEncoded'] = train['CompanyPrefix'].map(mean_company)\n",
    "val['CompanyEncoded'] = val['CompanyPrefix'].map(mean_company)\n",
    "test['CompanyEncoded'] = test['CompanyPrefix'].map(mean_company)\n",
    "\n",
    "# filling not existing companies in test with global mean\n",
    "global_mean = train['SalaryNormalized'].mean()\n",
    "val['CompanyEncoded'] = val['CompanyEncoded'].fillna(global_mean)\n",
    "test['CompanyEncoded'] = test['CompanyEncoded'].fillna(global_mean)\n",
    "\n",
    "train.drop(columns=['Company', 'CompanyPrefix'], inplace=True)\n",
    "val.drop(columns=['Company', 'CompanyPrefix'], inplace=True)\n",
    "test.drop(columns=['Company', 'CompanyPrefix'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_company.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79a33b",
   "metadata": {},
   "source": [
    "# 10. Tabular data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tab = train.drop(columns=['Title', 'FullDescription'])\n",
    "val_tab = val.drop(columns=['Title', 'FullDescription'])\n",
    "test_tab = test.drop(columns=['Title', 'FullDescription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9088e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tab.to_csv('data/train_preprocessed.csv', index=False)\n",
    "val_tab.to_csv('data/val_preprocessed.csv', index=False)\n",
    "test_tab.to_csv('data/test_preprocessed.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tab.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7434fd-43f6-4d29-9eb5-747a7e46d57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
